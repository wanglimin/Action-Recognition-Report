\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \hyphenpenalty=1000

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{2} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Towards Good Practices for Very Deep Two-Stream ConvNets}

\author{Limin Wang \quad \quad Yuanjun Xiong \quad \quad Zhe Wang \quad \quad Yu Qiao \\
\small Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong \\
\small Shenzhen key lab of Comp. Vis. \& Pat. Rec.,  Shenzhen Institutes of Advanced Technology, CAS, China \\
{\tt\small \{07wanglimin,bitxiong,buptwangzhe2012\}@gmail.com, yu.qiao@siat.ac.cn}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
  Human action recognition has become an important problem in computer vision and received a lot of research interests in this community \cite{SimonyanZ14,WangS13a,WangQT15a}. The problem of action recognition is very challenging due to the large intra-class variations, low video resolution, and high dimension of video data.

  The past several years have witnessed great progress on action recognition from short clips. These research works can be roughly categorized into two types. The first style algorithm focus on the \emph{hand-crafted} local features and \emph{Bag of Visual Worlds} (BoVWs) representation. The most successful example is to extract improved trajectory features \cite{WangS13a} and employ Fisher vector representation \cite{SanchezPMV13}. The second style algorithm utilizes \emph{deep convolutional networks} (ConvNets) to learn video representation from raw data (e.g. RGB images or optical flow fields) and train recognition system in an end-to-end manner. The most competitive deep model is the two-stream ConvNets \cite{SimonyanZ14}.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Human action recognition has become an important problem in computer vision and received a lot of research interests in this community \cite{SimonyanZ14,WangS13a,WangQT15a}. The problem of action recognition is very challenging due to the large intra-class variations, low video resolution, and high dimension of video data.

The past several years have witnessed great progress on action recognition from short clips. These research works can be roughly categorized into two types. The first style algorithm focus on the \emph{hand-crafted} local features and \emph{Bag of Visual Worlds} (BoVWs) representation. The most successful example is to extract improved trajectory features \cite{WangS13a} and employ Fisher vector representation \cite{SanchezPMV13}. The second style algorithm utilizes \emph{deep convolutional networks} (ConvNets) to learn video representation from raw data (e.g. RGB images or optical flow fields) and train recognition system in an end-to-end manner. The most competitive deep model is the two-stream ConvNets \cite{SimonyanZ14}.

However, unlike image classification \cite{KrizhevskySH12}, deep ConvNets did not yield significant improvement over these traditional methods. We argue that there are two reasons to explain this phenomenon. First, the concept of action is more complex than object and it is relevant to other high-level vision concepts, such as interacting object, scene context, human pose. Intuitively, the more complicated problem will need the model of higher complexity. However, the current two-stream ConvNets are relatively shallow (5 convolutional layers and 3 fully-connected layers) compared with those successful models in image classification \cite{SimonyanZ14a,SzegedyLJSRAEVR14}. Second, the dataset of action recognition is extremely small compared the ImageNet dataset. For example, the UCF101 dataset only contains $13,320$ clips. However, these deep ConvNets always require a huge number of training samples to tune the network weights.

In order to address these issues, this report presents very deep two-stream ConvNets for action recognition. Very deep two-stream ConvNets contain high modeling capacity and are capable of handling the large complexity of action classes. However, due to the second problem above, training very deep models in such a small dataset is much challenging due to the over-fitting problem. We propose several good practices to make the training of very deep two-stream ConvNets stable and reduce the effect of over-fitting. By carefully training our proposed very deep ConvNets on the action dataset, we are able to achieve the state-of-the-art performance on the dataset of UCF101.

The remainder of this report is organized as follows. In Section \ref{sec:method}, we introduce our proposed very deep two-stream ConvNets in details, including network architectures, training details, testing strategy. We report our experimental results on the dataset of UCF101 in Section \ref{sec:experiment}. Finally, we conclude our report in Section \ref{sec:conclusion}.

\section{Very Deep Two-stream ConvNets}
\label{sec:method}

In this section, we give a detailed description of our proposed method. We first introduce the architectures of very deep two-stream ConvNets. After that, we present the training details, which are very important to avoid the problem of over-fitting. Finally, we describe our testing strategies for action recognition.

\subsection{Network architectures}
Network architectures are of great importance in the design of deep ConvNets. In the past several years, many famous network structures have been proposed for image classification, such as AlexNet \cite{KrizhevskySH12}, ClarifaiNet \cite{ZeilerF14}, GoogLeNet \cite{SzegedyLJSRAEVR14}, VGGNet \cite{SimonyanZ14a}, and so on. Some trends emerge during the evolution of from AlexNet to VGGNet: smaller convolutional kernel size, smaller convolutional strides, and deeper network architectures. These trends have turned out to be effective on improving object recognition performance. However, their effect has not be investigated for action recognition in video domain. Here, we choose two latest successful network structures to design very deep two-stream ConvNets, namely GoogLeNet and VGGNet.

\textbf{GoogLeNet.} It is essentially a deep convolutional network architecture codenamed \emph{Inception},whose basic idea is  Hebbian principle and the intuition of multi-scale processing. An important component in Inception network is Inception module. Inception module is composed of multiple convolutional filters with different sizes alongside each other. In order to speed up the computational efficiency, $1 \times 1$ convolutional operation is chosen for dimension reduction. GoogLeNet is a 22-layer network consisting of Inception modules stacked upon each other, with occasional max-pooling layers with stride 2 to halve the resolution of grid. More details can be found in its original paper \cite{SzegedyLJSRAEVR14}.

\textbf{VGGNet.} It is a new convolutional architecture with smaller convolutional size ($3 \times 3$), smaller convolutional stride ($1 \times 1$), smaller pooling window ($2 \times 2$), deeper structure (up to 19 layers). The VGGNet systematically investigates the influence of network depth on the recognition performance, by building and pre-training deeper architectures based on the shallower ones. Finally, two successful network structures are proposed for the ImageNet challenge: VGG-16 (13 convolutional layers and 3 fully-connected layers) and VGG-19 (16 convolutional layers and 3 fully-connected layers). More details can be found in its original paper \cite{SimonyanZ14a}.


\textbf{Very Deep Two-stream ConvNets.} Following these successful architectures in object recognition, we adapt them to the design of two-stream ConvNets for action recognition in videos, which we called \emph{very deep two-stream ConvNets}. We empirically study both GoogLeNet and VGG-16 for very deep two-stream ConvNets. The spatial net is built on a single frame image ($224 \times 224 \times 3$) and therefore its architecture is the same as those for object recognition in image domain. The input of temporal net is 10-frame stacking of optical flow fields ($224 \times 224 \times 20$) and thus the convolutional size in the first layer is different from those in for image classification.

\subsection{Network training}
Here we describe how to train very deep two-stream ConvNets on the UCF101 dataset. The UCF101 dataset contains $13,320$ video clips and provides 3 splits for evaluation. For each split, there are $10,000$ clips for training and $3300$ clips for testing. As the training dataset is extremely small and the concept of action is relatively complex, learning very deep two-stream ConvNets is very challenging. From our empirical explorations, we discover several good practices for training very deep two-stream ConvNets as follows.

\textbf{Pre-training for Two-stream ConvNets.} Pre-training has turned out to be an effective way to initialize deep ConvNets when there is not enough training samples available. For spatial nets, as in \cite{SimonyanZ14}, we choose the ImageNet models as the initialization for network training. For temporal net, its input modality are optical flow fields, which capture the motion information and are different from static RGB images. Interestingly, we observe that it still works well by pre-training temporal nets with ImageNet model. In order to make this pre-training reasonable, we make several modifications on optical flow fields and ImageNet model. First, we extract optical flow fields for each video and discretize optical flow fields into interval of $[0,255]$ by a linear transformation. Second, as the channel number of input for temporal nets is different from that of spatial nets, we average the ImageNet model filters of first layer across the channel and then copy the average results $20$ times as the initialization of temporal nets.

\textbf{Smaller Learning Rate.} As we pre-trained the two-stream models with ImageNet model, we use a smaller learning rate compared with original training in \cite{SimonyanZ14}. Specifically, we set the learning rate as follows:
\begin{itemize}
  \item For temporal net, the learning rate starts with $0.005$, decreases to its 1/10 every 10,000 iterations, stops at 30,000 iterations.
  \item For spatial net, the learning rate starts with $0.001$, decreases to its 1/10 every 4,000 iterations, stops at 10,000 iterations.
\end{itemize}
In total, the learning rate is decreased 3 times. At the same time, we notice that it requires less iterations for the training of very deep two-stream ConvNets. We analyze that this may be due to the fact we pre-trained the very deep networks with the ImageNet models.

\textbf{More Data Augmentation Techniques.} It has been demonstrated that data augmentation such as random cropping and horizontal flipping is very effective to avoid the problem of over-fitting. Here, we use two data augmentation techniques for training very deep two-stream ConvNets as follows:
\begin{itemize}
  \item We design a corner cropping strategy, which means we only crop 4 corners and 1 center of the images. We find that if we use random cropping method, it is more likely select the regions close to the image center and goes quickly for over-fitting with the training dataset. However, if we constrain the cropping to the 4 corners or 1 center explicitly, the variations of input to the network will increase and it helps to reduce the effect of over-fitting.
  \item We use the multi-scale cropping method for very deep two-stream ConvNets training. Multi-scale representations have turned out to be effective for improving the performance of object recognition on the ImageNet dataset \cite{SimonyanZ14a}. Here, we adapt this good practice into the task of action recognition. But we present an efficient implementation compared with in object recognition \cite{SimonyanZ14a}. We fix the image size as $256 \times 340$ and randomly sample the cropping width and height from \{256,224,192,168\}. After that, we resize the cropped regions as $224 \times 224$. It is worth noting that this cropping strategy not only introduce the multi-scale augmentation, but also aspect ratio augmentation.
\end{itemize}

\textbf{High Dropout Ratio.} Similar to the original two-stream ConvNets \cite{SimonyanZ14}, we also set high drop out ratio for the fully connected layers in the very deep two-stream ConvNets. In particular, we set 0.9 and 0.8 drop out ratios for the fully connected layers of temporal nets. For spatial nets, we set 0.9 and 0.9 drop out ratios for the fully connected layers of spatial nets.

%Multi-GPU training 
\textbf{Multi-GPU training.}  One great obstacle for applying deep learning models in video recognition task is the prohibitively long training time. Also the input of multiple frames heightens the memory consumption for storing layer activations. We solve these problems by employing multiple GPUs in the data-parallel manner. 
During training, each worker GPU performs the forward and backward propagation in parallel. After each iteration, the gradients on the GPUs will be synchronized. This ensures that the training is conducted "as if" in a single process and the parallelism is transparent to the user.

Following a similar technique used in \cite{HeArxiv2015}, we avoid synchronizing the gradients of fully connected (fc) layers by gathering the activations from all worker processes before running the fc layers. 
A lightweight dependency tracking has also been introduced to trigger the gradient synchronization in background as soon as one layer completes its backward propagation.
This enables us to overlap data transmission and GPU computation, and further reduce communication overhead.

Due to these techniques, we have obtained appealing scalability from the system. With 4 NVIDIA Titan Black GPUs, the training is 3.9x faster and takes 3.3x less memory per GPU. With 8 GPUs, the training becomes 7 times faster. The system can automatically scale to multiple nodes with minimal effort. A reasonable speed up can be achieved with common Ethernet connection and maxed out with InfiniBand connections. The training system is implemented with Caffe and OpenMPI and publicly available at \url{https://github.com/yjxiong/caffe}. We expect to see broad use of the this system beyond the action recognition task.

\begin{table*}
\begin{center}
\resizebox{1\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
  &\multicolumn{4}{|c|}{Spatial nets} & \multicolumn{4}{c|}{Temporal nets} &  \multicolumn{4}{c|}{Two-stream ConvNets} \\ \hline
  Architecture & Split1 & Split2 & Split3 & Average & Split1 & Split2 & Split3 & Average & Split1 & Split2 & Split3 & Average \\ \hline
  Clarifai (from \cite{SimonyanZ14}) & 72.7\% & - & - & 73.0\% & 81.0\% & - & - &  83.7\% & 87.0\% & - & - &  88.0\% \\
  GoogLeNet & 77.1\% & 73.2\% & 75.6\% & 75.3\% & 83.9\% & 86.5\% & 86.9\% & 85.8\% & 89.0\% & 89.3\% & 89.5\% & 89.3\%\\
  VGGNet-16 & 79.8\% & 77.3\% & 77.8\% & {\bf 78.4\%} & 85.7\% & 88.2\% & 87.4\% & {\bf 87.0\%} & 90.9\% & 91.6\% & 91.6\% & {\bf 91.4\%} \\
  \hline
\end{tabular}
}
\vspace{2mm}
\caption{Performance comparison of different architectures on the UCF101 dataset.}
\label{tbl:result_ucf}
\end{center}
\end{table*}

\begin{table*}
\begin{center}
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
  \hline
  \multicolumn{3}{|c|}{Spatial nets} & \multicolumn{3}{c|}{Temporal nets}  \\ \hline
   ClarifaiNet & GoogLeNet & VGGNet-16 & ClarifaiNet & GoogLeNet & VGGNet-11 \\ \hline
   42.3\% & 53.7\% & \textbf{54.5\%} & \textbf{47.0\%} & 39.9\% &  42.6\%\\
  \hline
\end{tabular}
}
\vspace{2mm}
\caption{Performance comparison of different architectures on the THUMOS15 \cite{THUMOS15} validation dataset from \cite{WangWXQ15}.}
\label{tbl:result_thumos}
\end{center}
\end{table*}

\subsection{Network testing}

For fair comparison with the original two-stream ConvNets \cite{SimonyanZ14}, we follow their testing scheme for action recognition. At the test time, we sample 25 frame images or optical flow fields for the testing of spatial and temporal nets, respectively. From each of these selected frames, we obtain 10 inputs for very deep two-stream ConvNets, i.e. 4 corners, 1 center, and their horizontal flipping. The final prediction score is obtained by averaging across the sampled frames and their cropped regions. For the fusion of spatial and temporal nets, we use a weighted linear combination of their prediction scores, where the weight is set as $2$ for temporal net and $1$ for spatial net.

\section{Experiments}
\label{sec:experiment}

In order to verify the effectiveness of proposed very deep two-stream ConvNets, we conduct experiments on the UCF101 \cite{Soomro12} datasets. The UCF101 dataset contains 101 action classes and there are at least 100 video clips for each class. The whole dataset contains $13,320$ video clips, which are divided into 25 groups for each action category. We follow the evaluation scheme of the THUMOS13 challenge \cite{THUMOS13} and adopt the three training/testing splits for evaluation. We report the average recognition accuracy across classes over these three splits.

For the extraction of optical flow fields, we follow the work of TDD \cite{WangQT15a} and choose the TVL1 optical flow algorithm \cite{ZachPB07}. Specifically, we use the OpenCV implementation, due to its balance between accuracy and efficiency.

We report the action recognition performance in Table \ref{tbl:result_ucf}. We compare three different network architectures, namely Clarifai, GoogLeNet, and VGGNet-16. From these results, we see that the deeper architectures obtains better performance and VGGNet-16 achieves the best performance. For spatial nets, VGGNet-16 outperform shallow network by around $5\%$ and for temporal net, VGGNet-16 is better by around $4\%$. Very deep two-stream ConvNets outperform original two-stream ConvNets by $3.4\%$. 

It is worth noting in our previous experience \cite{WangWXQ15} in THUMOS15 Action Recognition Challenge \cite{THUMOS15}, we have tried very deep two-stream ConvNets and but temporal nets with deeper structure did not yield good performance. The performance of very deep stream ConvNets is shown in Table \ref{tbl:result_thumos}. For the THUMOS15 submission

\begin{table}
\begin{center}
\resizebox{0.35\textwidth}{!}{
\begin{tabular}{|c|c|}
  \hline
  Method & Accuracy \\
  \hline
  iDT+FV \cite{WangS13a} & 85.9\% \\
  iDT+HSV \cite{PengWWQ14} & 87.9\% \\
  MIFS+FV \cite{Lan15} & 89.1\% \\
  TDD+FV \cite{WangQT15a} & 90.3\% \\
  \hline
  DeepNet \cite{KarpathyTSLSF14} & 63.3\% \\
  Two-stream \cite{SimonyanZ14} & 88.0\% \\
  Two-stream+LSTM \cite{Ng15} & 88.6\% \\
  \hline
  Very deep two-stream & {\bf 91.4\%} \\
  \hline
\end{tabular}
}
\vspace{2mm}
\caption{Performance comparison with the state of the art on UCF101 dataset.}
\end{center}
\end{table}

\section{Conclusions}
\label{sec:conclusion}

{
\bibliographystyle{ieee}
\bibliography{deep}
}

\end{document}
